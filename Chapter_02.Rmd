---
title: "Chapter 02"
output:
  html_document:
    keep_md: yes
    toc: yes
---
```{r setup, include=FALSE}
library(ggplot2)
library(dplyr)
```
# Data
```{r data}
set.seed(1)
n = 100
D = data.frame(
  x = rnorm(n),
  y = rnorm(n)
)
```

# 2.1 Covariance

Deviation from Mean and Product of Deviations from the Mean:

```{r Deviaion}
D$devp = (D$x-mean(D$x)) * (D$y-mean(D$y))
sum(D$devp)/(n-1)
cov(D$x, D$y)
```

Covariance is the sum of the product of x and y's deviation from the mean divided by n-1.

```{r Covariance}
D$devp.col = ifelse(D$devp<=0, "black", "red")
ggplot(D, aes(x,y)) +
  geom_point(color=D$devp.col)
```

Sign of the product of the deviation of the mean roughly equal to the *direction* of the linear relationship but nothing about the *strength* of that relationship.

# 2.2 Correlation Coefficient

Standardize both X and Y:

```{r Standardize}
D$zx = (D$x-mean(D$x))/sd(D$x)
D$zy = (D$y-mean(D$y))/sd(D$y)
(sum(D$zx*D$zy))/(n-1)
cor(D$x, D$y)
```

Measures both the *direction* and *strength* of the linear relationship between variables.

# 2.4 Simple Linear Regression

> How do you know, in advance, if the relationship between X and Y is indeed
> ameanable to modeling with a linear equation?

Addressed on page 32: Visual inspection with scatter plots with least squares regression line plotted.

# 2.5 Parameter Estimation

```{r Parameter_Estimation}
COMPUTER = read.csv("./All_Data//P031.txt", sep="\t")
p=ggplot(COMPUTER, aes(Units, Minutes)) +
  geom_point()
print(p)

# Calc coefficients for the least squares regression line
hat_B_1 = sum( (COMPUTER$Minutes-mean(COMPUTER$Minutes))*(COMPUTER$Units - mean(COMPUTER$Units)) ) / sum( (COMPUTER$Units-mean(COMPUTER$Units))^2 )
hat_B_0 = mean(COMPUTER$Minutes) - hat_B_1*mean(COMPUTER$Units)

# Compute line
LSR = data.frame(hat_X = COMPUTER$Units)
LSR$hat_Y = hat_B_0 + hat_B_1*LSR$hat_X
p = p + geom_line(data=LSR, aes(hat_X, hat_Y))
print(LSR)
print(p)
```

With R's `lm()` function:
```{r LSR_with_LM}
COMPUTER.lm = lm(Minutes ~ Units, COMPUTER)
layout(matrix(1:4,2,2))
plot(COMPUTER.lm)
```

Explanation of plots from [Using R for Linear Regression](http://www.montefiore.ulg.ac.be/~kvansteen/GBIO0009-1/ac20092010/Class8/Using%20R%20for%20linear%20regression.pdf):

> The plot in the upper left shows the residual errors plotted versus their fitted values. The residuals should be randomly distributed around the horizontal line representing a residual error of zero; that is, there should not be a distinct trend in the distribution of points. The plot in the lower left is a standard Q-Q plot, which should suggest that the residual errors are normally distributed. The scale-location plot in the upper right shows the square root of the standardized residuals (sort of a square root of relative error) as a function of the fitted values. Again, there should be no obvious trend in this plot. Finally, the plot in the lower right shows each points leverage, which is a measure of its importance in determining the regression result. Superimposed on the plot are contour lines for the Cook’s distance, which is another measure of the importance of each observation to the regression. Smaller distances means that removing the observation has little affect on the regression results. Distances larger than 1 are suspicious and suggest the presence of a possible outlier or a poor model.

```{r Plot_COMPUTER.lm}
ggplot(COMPUTER, aes(Units, Minutes)) +
  geom_point() +
  geom_line(data=fortify(COMPUTER.lm), aes(Units, .fitted))
```

# 2.6 Test of Hypothesis

How do we know if X is actually a useful predictor of Y?

* In $y_i=\beta_0 + \beta_1*X_i + \varepsilon_i$ we would know that $X$ and $Y$ do not have a linear relation  if $\beta_1=0$.
* If the above is true then for each X the resulting $\varepsilon_i$ will be independent, random, with a mean of zero, and an unknown common variance $\sigma^2$.

_long explanation..._

* degrees of freedom: Number of observations minus the number of estimated coefficients in the linear regression.

Statement of hypothesis:

* $H_0: \beta_1=0$ _meaning_ X and Y do not have a linear relation
* $H_1: \beta_1\not=0$ _meaning_ X and Y have some linear relation

Test with Student's t test. Only accept the null hypothesis ($H_0$) if:

$|t_1| \ge t_{(n-2,\alpha/2)}$

Calculated value of t-test is greater than or equal to the value from the table for a 2-degrees of freedom, two-sided test with a significance level of $\alpha$.

Alternate expression of the criteria:

$p|t_1| \le \alpha$

ie. the _p-value_ is less than the significance level.

## t-test with Computer Example Data

Dilbert's boss estimates that each additional component adds 12 minutes to the repair time. Does the data support this intuition?

* $H_0$: _Yes_ which implies $\beta_1 = 12$
* $H_1$: _No_ which implies $\beta_1 \ne 12$

Spreadsheet Bob would estimate off the data: $\overline{(\frac{minutes}{units})}$ = `r mean(COMPUTER$Minutes / COMPUTER$Units)`. I'd be inclined to agree since there is no instance of a repair in the data where minutes-per-unit was less than 12. But look at the minutes added per each additional unit, on average:

```{r min_per_unit}
COMPUTER %>%
  group_by(Units) %>%
  summarise(MeanMinutes=mean(Minutes), MinPerUnit=sum(Minutes)/sum(Units)) %>%
  mutate(DeltaMeanMinutes=MeanMinutes-lag(MeanMinutes))
```

Eh... maybe the boss was correct? 5 cases where the delta is near 12, 4 where it isn't close.

Use the t-test:

* $\hat\sigma^2$ = Calc sum-of-the-square-errors (SSE) divided by the degrees of freedom:

```{r Sigma2}
hat_sigma2 = sum( (COMPUTER$Minutes - LSR$hat_Y)^2 ) / (nrow(COMPUTER)-2)
se_hat_beta1 = sqrt(hat_sigma2)/sqrt(sum( (COMPUTER$Units-mean(COMPUTER$Units))^2 ))
```

Therefore:

$t_1 = \frac{\hat\beta_1-12}{s.e.(\hat\beta_1)} = \frac{`r hat_B_1`-12}{`r se_hat_beta1`} = `r (hat_B_1-12)/se_hat_beta1`$

Here the book references an unknown table of t-test results that claims $t_{(n-2,\alpha/2)}=t_{12,0.25)}=2.18$ and that since $t_1 = `r (hat_B_1-12)/se_hat_beta1` \gt 2.18$ that the null hypothesis must be rejected and $\beta_1 \ne 12$.

### ala R

```{r Management_Prediction}
COMPUTER_EST = rbind( 
  # Actual observations
  mutate(COMPUTER, dataset="observed"),

  # Management prediction
  COMPUTER %>%
    mutate(
      dataset = "management",
      Minutes = Units*12 + hat_B_0
    )
)
print(COMPUTER_EST)

ggplot() +
  geom_line(data=fortify(COMPUTER.lm), aes(Units, .fitted), color="blue") +
  geom_point(data=COMPUTER_EST[COMPUTER_EST$dataset=="observed",], aes(Units, Minutes)) +
  geom_line(data=COMPUTER_EST[COMPUTER_EST$dataset=="management",], aes(Units, Minutes), color="red") +
  ylab("Minutes")

mgmt.ttest = t.test(Minutes ~ dataset, data=COMPUTER_EST, paired=TRUE)
print(mgmt.ttest)
```

* Per `?t.test` and [Student's t-Test](http://en.wikipedia.org/wiki/Student%27s_t-test) it seems appropriate to treat observed data and the management claim as a paired set of samples Therefore `paired=TRUE` is appropriate.

> Huh... not clear if I'm doing this right.

## Test Using Correlation Coefficient

$t_1 = \frac{Cor(Y,X)\sqrt{n-2}}{\sqrt{1-(Cor(Y,X))^2}}$

```{r Test_Using_Cor}
(cor(COMPUTER$Minutes, COMPUTER$Units)*sqrt(nrow(COMPUTER-2))) / (sqrt(1-cor(COMPUTER$Minutes, COMPUTER$Units)^2))
```

Not the same as $t_1=`r (hat_B_1-12)/se_hat_beta1`$ above. Not following.

# 2.8 Predictions

```{r Predict}
predict(COMPUTER.lm, data.frame(Units=4))
```

# 2.9 Measuring Quality of Fit

```{r Quality}
print(summary(COMPUTER.lm))
```

Evidence of a good fit for this simple model:

* t-Value for the predictor Units (aka. $\hat\beta_1$) is large. (relative to what, exactly?)
* The two-sided p-value is very small. [Using R for Linear Regression](http://www.montefiore.ulg.ac.be/~kvansteen/GBIO0009-1/ac20092010/Class8/Using%20R%20for%20linear%20regression.pdf) has a good explanation of what the _Pr(>|t|) value means:

> Estimates for the model’s coefficients are provided along with the their (sic) standard deviations (‘Std Error’), and a t-value and probability for a null hypothesis that the coefficients have values of zero.

* $R^2$ is close to 1 indicating that almost all of the variance in the response variable is explained by the model.

# Homework

## 2.10 - Husbands and Wives

```{r Problem_2.10}
HW=read.table("All_Data//P052.txt", sep="\t", header=TRUE)
p=ggplot(HW, aes(Wife, Husband)) + geom_point()
print(p)

# a - cov of husbands and wives. Positive covariance.
cov(HW$Husband, HW$Wife)

# b - cov in inches
cov(HW$Husband/2.54, HW$Wife/2.54)

# c - cor of husbands and wives
cor(HW$Husband, HW$Wife)

# d - cor in inches
cor(HW$Husband/2.54, HW$Wife/2.54)

# e - cor if every man married a woman 5cm shorter == 1
cor(HW$Husband, HW$Husband-5)

# f - The response variable could be either the husband's or wife's height.
# Lets go with the Husband's height as the respose variable
# g - Test f with H0: slope == 0
HW.lm = lm(Husband ~ Wife, HW)
print(p + geom_line(data=fortify(HW.lm), aes(Wife, .fitted)))
HW.lm.summary = summary(HW.lm)
print(HW.lm.summary)
```

## 2.12 - Newspapers

```{r Newspapers}
NEWS = read.csv("All_Data//P054.txt", sep="\t", header=TRUE)

# a - scatter plot
p = ggplot(NEWS, aes(Daily, Sunday)) + geom_point()
print(p)

# b - fit
NEWS.lm = lm(Sunday ~ Daily, NEWS)
print(summary(NEWS.lm))
print(p + geom_line(data=fortify(NEWS.lm), aes(Daily, .fitted)))

# c - Confidence Interval
confint(NEWS.lm) # Estimate +/- Std. Error * t-value

# d - Is there a significant relationship between Daily and Sunday circulation?
# Yes?

# e - 95% confidence interval for Daily with circulation=500,000
predict(NEWS.lm, data.frame(Daily=500000), interval="confidence", level=.95)

# g - how is this different?

# h - 95% confidence interval for Daily with circulation=2e6
predict(NEWS.lm, data.frame(Daily=2e6), interval="confidence", level=.95)
# Not likely to be accurate. Outside of the bounds of training data.
```